name: Website Grammar Check

on:
  repository_dispatch:
    types: [grammar-check]
  workflow_dispatch:
    inputs:
      url:
        description: "URL to check (optional if payload supplies it)"
        required: false
        type: string
      text:
        description: "Raw text to check (optional)"
        required: false
        type: string

permissions:
  contents: read
  issues: write

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install tools
        run: |
          sudo apt-get update
          sudo apt-get install -y jq lynx

      - name: Extract payload and build FULL text
        id: build
        run: |
          set -euo pipefail
          URL=$(jq -r '.client_payload.url // empty' "$GITHUB_EVENT_PATH")
          TEXT=$(jq -r '.client_payload.text // empty' "$GITHUB_EVENT_PATH")
          CID=$(jq -r '.client_payload.client_id // empty' "$GITHUB_EVENT_PATH")
          # allow manual run inputs too
          if [ -z "$URL" ];  then URL="${{ github.event.inputs.url }}";  fi
          if [ -z "$TEXT" ]; then TEXT="${{ github.event.inputs.text }}"; fi

          echo "SRC_URL=$URL"   >> $GITHUB_ENV
          echo "CLIENT_ID=$CID" >> $GITHUB_ENV

          if [ -n "$URL" ]; then
            echo "Fetching $URL"
            curl -fsSL -H "User-Agent: Mozilla/5.0" "$URL" -o page.html
            lynx -dump -nolist page.html > doc_full.txt
          else
            printf "%s" "$TEXT" > doc_full.txt
          fi

          if [ ! -s doc_full.txt ]; then
            echo "::error::No content to analyze (neither URL nor text)"; exit 1
          fi

          BYTES=$(wc -c < doc_full.txt)
          echo "FULL_BYTES=$BYTES" >> $GITHUB_ENV
          echo "Built doc_full.txt ($BYTES bytes)"

      - name: Chunk long text (~15 KB per chunk)
        run: |
          set -euo pipefail
          rm -rf chunks reports
          mkdir -p chunks reports
          # Split by bytes for robustness; 15000 stays within LT public limits
          split -b 15000 -d -a 3 doc_full.txt chunks/chunk_
          COUNT=$(ls chunks | wc -l | tr -d ' ')
          echo "CHUNK_COUNT=$COUNT" >> $GITHUB_ENV
          echo "Created $COUNT chunk(s)"

      - name: Run LanguageTool on all chunks
        run: |
          set -euo pipefail
          for f in $(ls chunks | sort); do
            fp="chunks/$f"
            echo "Checking $fp"
            HTTP=$(curl -sS -o "reports/$f.json" -w "%{http_code}" \
              -X POST https://api.languagetool.org/v2/check \
              -H 'Content-Type: application/x-www-form-urlencoded' \
              -d 'language=en-US' \
              --data-urlencode text@"$fp")
            if [ "$HTTP" -ne 200 ]; then
              echo "::warning::LanguageTool returned $HTTP for $fp (using empty matches)"
              echo '{"matches":[]}' > "reports/$f.json"
            else
              if ! jq -e . "reports/$f.json" >/dev/null 2>&1; then
                echo "::warning::Non-JSON from LanguageTool for $fp (using empty matches)"
                echo '{"matches":[]}' > "reports/$f.json"
              fi
            fi
            sleep 1  # gentle throttle for the public API
          done
          # Aggregate all matches from all chunks into a single array
          jq -s '[.[] | .matches] | flatten' reports/*.json > matches_all.json
          COUNT=$(jq 'length' matches_all.json)
          echo "COUNT=$COUNT" >> $GITHUB_ENV
          echo "Aggregated $COUNT match(es)"

      - name: Build Markdown report
        run: |
          {
            echo "# Grammar Report"
            if [ -n "${SRC_URL}"   ]; then echo "**URL:** ${SRC_URL}  "; fi
            if [ -n "${CLIENT_ID}" ]; then echo "**Client ID:** ${CLIENT_ID}  "; fi
            echo "**Bytes scanned (full):** ${FULL_BYTES}  "
            echo "**Chunks:** ${CHUNK_COUNT}  "
            echo "**Issues found:** ${COUNT}"
            echo
            echo "## Top findings"
            jq -r '
              .[0:100][] |
              "- **\(.rule.issueType // .rule.category.id // .rule.id)**: \(.message)
                 - Context: \"\(.context.text)\"
                 - Suggestion: \((.replacements[0].value // "—"))"
            ' matches_all.json
          } > report.md
          echo "Report written to report.md"

      - name: Create issue with report
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const url = process.env.SRC_URL || '(manual input)';
            const title = `Grammar report – ${url} – ${new Date().toISOString().slice(0,19)}Z`;
            const body  = fs.readFileSync('report.md','utf8');
            const labels = ['grammar-report'];
            const { data: issue } = await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title, body, labels
            });
            core.info(`Created issue #${issue.number}`);

      # ---- OPTIONAL Google Doc callback; follows Apps Script 302 redirect ----
      - name: Post summary to Google Doc (optional)
        env:
          DOC_CALLBACK_URL: ${{ secrets.DOC_CALLBACK_URL }}
          DOC_CALLBACK_KEY: ${{ secrets.DOC_CALLBACK_KEY }}
          SRC_URL: ${{ env.SRC_URL }}
          CLIENT_ID: ${{ env.CLIENT_ID }}
          COUNT: ${{ env.COUNT }}
        run: |
          set -euo pipefail
          if [ -z "${DOC_CALLBACK_URL:-}" ] || [ -z "${DOC_CALLBACK_KEY:-}" ]; then
            echo "Doc callback secrets not set; skipping."
            exit 0
          fi
      
          # Use GET + query params; follow 302 automatically
          HTTP=$(curl -sS -L -G -o /tmp/doc_cb.out -w "%{http_code}" \
            "$DOC_CALLBACK_URL" \
            --data-urlencode "api_key=${DOC_CALLBACK_KEY}" \
            --data-urlencode "url=${SRC_URL:-}" \
            --data-urlencode "client_id=${CLIENT_ID:-}" \
            --data-urlencode "audit_type=single" \
            --data-urlencode "total=${COUNT:-0}")
      
          echo "Callback HTTP: $HTTP"
          if [ "$HTTP" -lt 200 ] || [ "$HTTP" -ge 300 ]; then
            echo "::error::Doc callback failed (HTTP $HTTP). Response:"
            sed -n '1,200p' /tmp/doc_cb.out
            exit 1
          fi

    echo "Doc callback OK:"
    sed -n '1,200p' /tmp/doc_cb.out
